{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LAB ASSIGNMENT - 3**"
      ],
      "metadata": {
        "id": "agC7DVztySwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Insert documents:**"
      ],
      "metadata": {
        "id": "Ychhc2qTw9Yu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSftlsPuuIU",
        "outputId": "7b8d3987-d419-4859-ee7f-5b02a5fbcf76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents inserted successfully:\n",
            "\n",
            "Doc 0: The quick brown fox jumps over the lazy dog.\n",
            "Doc 1: The lazy dog sleeps.\n",
            "Doc 2: A quick brown dog outpaces a quick fox.\n"
          ]
        }
      ],
      "source": [
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog sleeps.\",\n",
        "    \"A quick brown dog outpaces a quick fox.\"\n",
        "]\n",
        "\n",
        "print(\"Documents inserted successfully:\\n\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Doc {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Preprocessing:**"
      ],
      "metadata": {
        "id": "ZLAX39HTxErZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess(doc):\n",
        "    doc = doc.lower()\n",
        "    doc = re.sub(r'[^a-z\\s]', '', doc)\n",
        "    tokens = doc.split()\n",
        "    return tokens\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "print(\"Preprocessed Documents:\\n\")\n",
        "for i, doc in enumerate(documents):\n",
        "    tokens = preprocess(doc)\n",
        "    processed_docs.append(tokens)\n",
        "    print(f\"Doc {i} tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9HYh3X8wYzb",
        "outputId": "f9299ee5-5df3-4f51-d81c-7b7bd266e009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Documents:\n",
            "\n",
            "Doc 0 tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Doc 1 tokens: ['the', 'lazy', 'dog', 'sleeps']\n",
            "Doc 2 tokens: ['a', 'quick', 'brown', 'dog', 'outpaces', 'a', 'quick', 'fox']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Positional Indexing:**"
      ],
      "metadata": {
        "id": "665V7MKjxNWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_positional_index(processed_docs):\n",
        "    positional_index = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    for doc_id, tokens in enumerate(processed_docs):\n",
        "        for position, term in enumerate(tokens):\n",
        "            positional_index[term][doc_id].append(position)\n",
        "\n",
        "    return positional_index\n",
        "\n",
        "positional_index = build_positional_index(processed_docs)\n",
        "\n",
        "print(\"Positional Index inserted successfully\\n\")\n",
        "for term, postings in positional_index.items():\n",
        "  print(term, \":\", dict(postings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkfL4hnTwyXs",
        "outputId": "6cb8669b-b4a7-4859-9fab-beec2e80460b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional Index inserted successfully\n",
            "\n",
            "the : {0: [0, 6], 1: [0]}\n",
            "quick : {0: [1], 2: [1, 6]}\n",
            "brown : {0: [2], 2: [2]}\n",
            "fox : {0: [3], 2: [7]}\n",
            "jumps : {0: [4]}\n",
            "over : {0: [5]}\n",
            "lazy : {0: [7], 1: [1]}\n",
            "dog : {0: [8], 1: [2], 2: [3]}\n",
            "sleeps : {1: [3]}\n",
            "a : {2: [0, 5]}\n",
            "outpaces : {2: [4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Phrasal Query Search:**"
      ],
      "metadata": {
        "id": "QeKCcGRdxpv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_query_positions(query, positional_index):\n",
        "    query_terms = query.lower().split()\n",
        "    result = {}\n",
        "\n",
        "    if query_terms[0] not in positional_index:\n",
        "        return result\n",
        "\n",
        "    for doc_id in positional_index[query_terms[0]]:\n",
        "        start_positions = positional_index[query_terms[0]][doc_id]\n",
        "        valid_positions = []\n",
        "\n",
        "        for pos in start_positions:\n",
        "            match = True\n",
        "            for i in range(1, len(query_terms)):\n",
        "                term = query_terms[i]\n",
        "                if term not in positional_index:\n",
        "                    match = False\n",
        "                    break\n",
        "                if (pos + i) not in positional_index[term].get(doc_id, []):\n",
        "                    match = False\n",
        "                    break\n",
        "\n",
        "            if match:\n",
        "                valid_positions.append(pos)\n",
        "\n",
        "        if valid_positions:\n",
        "            result[doc_id] = valid_positions\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Take phrase query input\n",
        "query = input(\"Enter phrasal query: \")\n",
        "\n",
        "positions = phrase_query_positions(query, positional_index)\n",
        "\n",
        "if positions:\n",
        "    print(\"\\nPhrase found at positions:\")\n",
        "    for doc_id, pos_list in positions.items():\n",
        "        print(f\"Doc {doc_id} → Positions: {pos_list}\")\n",
        "else:\n",
        "    print(\"\\nPhrase not found in any document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj0I8WJKxiP_",
        "outputId": "3855fe20-94ae-4a5d-c68a-51a52989787a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter phrasal query: lazy dog\n",
            "\n",
            "Phrase found at positions:\n",
            "Doc 0 → Positions: [7]\n",
            "Doc 1 → Positions: [1]\n"
          ]
        }
      ]
    }
  ]
}